{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8190591,
          "sourceType": "datasetVersion",
          "datasetId": 4850432
        },
        {
          "sourceId": 8303996,
          "sourceType": "datasetVersion",
          "datasetId": 4933043
        },
        {
          "sourceId": 8304151,
          "sourceType": "datasetVersion",
          "datasetId": 4933099
        },
        {
          "sourceId": 8304593,
          "sourceType": "datasetVersion",
          "datasetId": 4933272
        },
        {
          "sourceId": 8407265,
          "sourceType": "datasetVersion",
          "datasetId": 5003045
        },
        {
          "sourceId": 11827255,
          "sourceType": "datasetVersion",
          "datasetId": 7429869
        },
        {
          "sourceId": 11852088,
          "sourceType": "datasetVersion",
          "datasetId": 7447356
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavitra-khare/DA6401_ASS3_withAtten/blob/main/ASS3_WithAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and login"
      ],
      "metadata": {
        "id": "VejTsKhNLm2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install wandb\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.179375Z",
          "iopub.execute_input": "2025-05-17T23:47:22.179660Z",
          "iopub.status.idle": "2025-05-17T23:47:22.184105Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.179643Z",
          "shell.execute_reply": "2025-05-17T23:47:22.183469Z"
        },
        "id": "ea9Qw2XX03Tv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "torch.cuda.empty_cache()\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#function to create 3*3 grid of heatMap\n",
        "import matplotlib.font_manager as fm\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.200542Z",
          "iopub.execute_input": "2025-05-17T23:47:22.200750Z",
          "iopub.status.idle": "2025-05-17T23:47:22.206313Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.200735Z",
          "shell.execute_reply": "2025-05-17T23:47:22.205543Z"
        },
        "id": "mUM6KgAM-oT1",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"7ff4b917f4a107c44e50219b5fd09ece84a34600\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.207252Z",
          "iopub.execute_input": "2025-05-17T23:47:22.207436Z",
          "iopub.status.idle": "2025-05-17T23:47:22.223262Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.207422Z",
          "shell.execute_reply": "2025-05-17T23:47:22.222649Z"
        },
        "id": "KZ1QyWsV-oUN",
        "outputId": "0fabf021-ae04-485c-b227-4938cbd538b7",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 46,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "HwQHFp9POlu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_0(filepath):\n",
        "    def process_row(text_row):\n",
        "        return list(text_row)\n",
        "\n",
        "    def collect_characters(reader_obj):\n",
        "        all_chars = []\n",
        "        for entry in reader_obj:\n",
        "            all_chars += process_row(entry[0])\n",
        "        return all_chars\n",
        "\n",
        "    with open(filepath, mode='r') as file_handle:\n",
        "        csv_reader = csv.reader(file_handle,delimiter='\\t')\n",
        "        return collect_characters(csv_reader)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.224464Z",
          "iopub.execute_input": "2025-05-17T23:47:22.224723Z",
          "iopub.status.idle": "2025-05-17T23:47:22.237513Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.224699Z",
          "shell.execute_reply": "2025-05-17T23:47:22.236914Z"
        },
        "id": "lL5546mp03Tw",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "'''Location of your CSV file (Extracted file)\n",
        "Location of your CSV file if on kaggle than zip file is fine'''\n",
        "\n",
        "trainFilepath=\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "valFilePath=\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "testFilePath=\"/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.238104Z",
          "iopub.execute_input": "2025-05-17T23:47:22.238276Z",
          "iopub.status.idle": "2025-05-17T23:47:22.252379Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.238262Z",
          "shell.execute_reply": "2025-05-17T23:47:22.251733Z"
        },
        "id": "R-OebR_K-oUE",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "chars = read_file_0(trainFilepath)\n",
        "setChar=set(chars)\n",
        "setChar.add('|')\n",
        "setOfchar = list(setChar)\n",
        "\n",
        "# Create the association between characters and their corresponding integer indices\n",
        "char_to_idx_latin= {char: i+1 for i, char in enumerate(setOfchar)}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.253939Z",
          "iopub.execute_input": "2025-05-17T23:47:22.254212Z",
          "iopub.status.idle": "2025-05-17T23:47:22.319304Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.254193Z",
          "shell.execute_reply": "2025-05-17T23:47:22.318601Z"
        },
        "id": "MuY2sxX_03Tx",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_1(trainFilepath):\n",
        "    def extract_chars(text):\n",
        "        return list(text)\n",
        "\n",
        "    def accumulate(reader_obj):\n",
        "        buffer = []\n",
        "        for entry in reader_obj:\n",
        "            buffer += extract_chars(entry[1])\n",
        "        return buffer\n",
        "\n",
        "    with open(trainFilepath, 'r') as file_stream:\n",
        "        csv_rows = csv.reader(file_stream,delimiter='\\t')\n",
        "        return accumulate(csv_rows)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.320049Z",
          "iopub.execute_input": "2025-05-17T23:47:22.320277Z",
          "iopub.status.idle": "2025-05-17T23:47:22.327366Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.320259Z",
          "shell.execute_reply": "2025-05-17T23:47:22.326491Z"
        },
        "id": "_Fu5-CC603Tx",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "maxLenDev=0\n",
        "\n",
        "chars = read_file_1(trainFilepath)\n",
        "setChar=set(chars)\n",
        "setChar.add('|')\n",
        "setOfchar = list(setChar)\n",
        "\n",
        "charToIndLang ={char: i+1 for i, char in enumerate(setOfchar)}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.328114Z",
          "iopub.execute_input": "2025-05-17T23:47:22.328287Z",
          "iopub.status.idle": "2025-05-17T23:47:22.417490Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.328273Z",
          "shell.execute_reply": "2025-05-17T23:47:22.416799Z"
        },
        "id": "BsSPFp2f-oUF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open(trainFilepath, 'r') as csv_file:\n",
        "    reader = csv.reader(csv_file,delimiter='\\t')\n",
        "    max_length = 0\n",
        "\n",
        "    def update_max(current_max, candidate):\n",
        "        return candidate if candidate > current_max else current_max\n",
        "\n",
        "    for record in reader:\n",
        "        length = len(record[0])\n",
        "        max_length = update_max(max_length, length)\n",
        "\n",
        "    maxLenEng = max_length"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.418252Z",
          "iopub.execute_input": "2025-05-17T23:47:22.418468Z",
          "iopub.status.idle": "2025-05-17T23:47:22.464237Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.418447Z",
          "shell.execute_reply": "2025-05-17T23:47:22.463772Z"
        },
        "id": "YnH-KUwf-oUH",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open(trainFilepath, 'r') as csv_stream:\n",
        "    reader_obj = csv.reader(csv_stream,delimiter='\\t')\n",
        "    longest = 0\n",
        "\n",
        "    def get_larger(a, b):\n",
        "        return b if b > a else a\n",
        "\n",
        "    for line in reader_obj:\n",
        "        current_length = len(line[1])\n",
        "        longest = get_larger(longest, current_length)\n",
        "\n",
        "    maxLenDev = longest"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T23:47:22.464779Z",
          "iopub.execute_input": "2025-05-17T23:47:22.464938Z",
          "iopub.status.idle": "2025-05-17T23:47:22.508226Z",
          "shell.execute_reply.started": "2025-05-17T23:47:22.464926Z",
          "shell.execute_reply": "2025-05-17T23:47:22.507766Z"
        },
        "id": "_CT0LTQL-oUI",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "characters in words -> indices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b_bIo2lzFaEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_characters_to_indices(word, dictionary):\n",
        "    def safe_lookup(char):\n",
        "        return dictionary[char] if char in dictionary else -1\n",
        "\n",
        "    mapped = [safe_lookup(ch) for ch in word]\n",
        "    filtered = list(filter(lambda idx: idx >= 0, mapped))\n",
        "    return filtered"
      ],
      "metadata": {
        "id": "WQAfkcuIEWhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_sequence_length(indices, maximumLength):\n",
        "    current_length = len(indices)\n",
        "\n",
        "    def trim(seq, length):\n",
        "        return seq[:length]\n",
        "\n",
        "    def pad(seq, total_length):\n",
        "        padding_needed = total_length - len(seq)\n",
        "        return seq + [0] * padding_needed\n",
        "\n",
        "    if current_length > maximumLength:\n",
        "        return trim(indices, maximumLength)\n",
        "    elif current_length < maximumLength:\n",
        "        return pad(indices, maximumLength)\n",
        "    return indices\n"
      ],
      "metadata": {
        "id": "KRHpQjLVEgUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_indices_to_tensor(indices, dictionary):\n",
        "    token = dictionary.get('|', 0)\n",
        "\n",
        "    def add_delimiters(seq, token_id):\n",
        "        return [token_id] + seq + [token_id]\n",
        "\n",
        "    sequence = add_delimiters(indices, token)\n",
        "    tensor_obj = torch.tensor(sequence, device=device)\n",
        "    return tensor_obj"
      ],
      "metadata": {
        "id": "JdcLlSlxEkVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_word_to_indices(word, maximumLength, dict):\n",
        "    char_ids = convert_characters_to_indices(word, dict)\n",
        "\n",
        "    def standardize_length(seq, target_len):\n",
        "        return adjust_sequence_length(seq, target_len)\n",
        "\n",
        "    resized = standardize_length(char_ids, maximumLength)\n",
        "    tensor_out = convert_indices_to_tensor(resized, dict)\n",
        "    return tensor_out\n"
      ],
      "metadata": {
        "id": "KOAvEkDaEohD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_tensor_to_generated_sequences(sequence):\n",
        "    tokens = sequence.split()\n",
        "\n",
        "    def concatenate(tokens):\n",
        "        return ''.join(tokens)\n",
        "\n",
        "    def compute_total_length(tokens):\n",
        "        return sum(len(token) for token in tokens)\n",
        "\n",
        "    combined = concatenate(tokens)\n",
        "    total_len = compute_total_length(tokens)\n",
        "\n",
        "    return combined, total_len"
      ],
      "metadata": {
        "id": "Zn_LBRPvEubI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assemble_tensor(final_tensor, partition_size=1):\n",
        "    def safe_partition_size(size):\n",
        "        return max(1, size)\n",
        "\n",
        "    chunk_size = safe_partition_size(partition_size)\n",
        "    segments = [final_tensor[idx:idx + chunk_size] for idx in range(0, len(final_tensor), chunk_size)]\n",
        "\n",
        "    return segments\n"
      ],
      "metadata": {
        "id": "FlncquPZFEDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assemble_assigned_generated_seq(path):\n",
        "    combined_seq, total_chars = assign_tensor_to_generated_sequences(path)\n",
        "\n",
        "    def determine_chunk_size(length, divisor=4):\n",
        "        return max(1, length // divisor)\n",
        "\n",
        "    segment_length = determine_chunk_size(total_chars)\n",
        "    partitioned = assemble_tensor(combined_seq, segment_length)\n",
        "\n",
        "    return partitioned\n"
      ],
      "metadata": {
        "id": "O2QcDz6iFEs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_indices(row):\n",
        "    src_text = row[0]\n",
        "    tgt_text = row[1]\n",
        "\n",
        "    def build_indexed_tensor(text, max_len, char_map):\n",
        "        return convert_word_to_indices(text, max_len, char_map)\n",
        "\n",
        "    source_tensor = build_indexed_tensor(src_text, maxLenEng, char_to_idx_latin)\n",
        "    target_tensor = build_indexed_tensor(tgt_text, maxLenDev, charToIndLang)\n",
        "\n",
        "    return source_tensor, target_tensor\n"
      ],
      "metadata": {
        "id": "At2QIJpuFHG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_v = []\n",
        "\n",
        "with open(valFilePath, 'r') as val_file:\n",
        "    csv_reader = csv.reader(val_file,delimiter='\\t')\n",
        "\n",
        "    def process_and_append(pairs, record):\n",
        "        eng_tensor, hin_tensor = generate_indices(record)\n",
        "        pairs.append([eng_tensor, hin_tensor])\n",
        "\n",
        "    for entry in csv_reader:\n",
        "        process_and_append(pairs_v, entry)\n"
      ],
      "metadata": {
        "id": "YKvNrnMmF5km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_t = []\n",
        "\n",
        "with open(testFilePath, 'r') as test_file:\n",
        "    test_reader = csv.reader(test_file,delimiter='\\t')\n",
        "\n",
        "    def append_indexed_pair(container, data_row):\n",
        "        src_tensor, tgt_tensor = generate_indices(data_row)\n",
        "        container.append([src_tensor, tgt_tensor])\n",
        "\n",
        "    for record in test_reader:\n",
        "        append_indexed_pair(pairs_t, record)\n"
      ],
      "metadata": {
        "id": "ZeBi9C_pF6U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "\n",
        "with open(trainFilepath, 'r') as train_file:\n",
        "    train_reader = csv.reader(train_file,delimiter='\\t')\n",
        "\n",
        "    def process_row_and_store(storage, row_data):\n",
        "        input_tensor, target_tensor = generate_indices(row_data)\n",
        "        storage.append([input_tensor, target_tensor])\n",
        "\n",
        "    for entry in train_reader:\n",
        "        process_row_and_store(pairs, entry)\n"
      ],
      "metadata": {
        "id": "uUnGKUqpF8TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA LOADER"
      ],
      "metadata": {
        "id": "NF2e0ahQGVRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_shuffle = True\n",
        "eval_shuffle = False\n",
        "\n",
        "def create_loader(dataset, size, shuffle_flag):\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=size, shuffle=shuffle_flag)\n",
        "\n",
        "dataloaderTrain = create_loader(pairs, batch_size, train_shuffle)\n",
        "dataloaderVal = create_loader(pairs_v, batch_size, eval_shuffle)\n",
        "dataloaderTest = create_loader(pairs_t, batch_size, eval_shuffle)\n"
      ],
      "metadata": {
        "id": "vyGDxtd5GCT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLASS FOR ENCODER DECODER and seq2seq"
      ],
      "metadata": {
        "id": "RBYwFz4DGZjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttnEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, embed_dim, rnn_type, dropout, num_layers, is_bidirectional):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "\n",
        "        rnn_map = {\n",
        "            \"GRU\": nn.GRU,\n",
        "            \"LSTM\": nn.LSTM,\n",
        "            \"RNN\": nn.RNN\n",
        "        }\n",
        "        self.rnn = rnn_map[rnn_type](embed_dim, hidden_dim, dropout=dropout, num_layers=num_layers, bidirectional=is_bidirectional)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, embed_dim, rnn_type, dropout, num_layers, is_bidirectional, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len + 2\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "\n",
        "        self.attn = nn.Linear(hidden_dim + embed_dim, self.max_seq_len)\n",
        "\n",
        "        concat_dim = hidden_dim * (2 if is_bidirectional else 1) + embed_dim\n",
        "        self.attn_combine = nn.Linear(concat_dim, hidden_dim)\n",
        "\n",
        "        rnn_input_dim = hidden_dim\n",
        "        rnn_cls = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[rnn_type]\n",
        "        self.rnn = rnn_cls(rnn_input_dim, hidden_dim, dropout=dropout, num_layers=num_layers, bidirectional=is_bidirectional)\n",
        "\n",
        "        output_proj_dim = hidden_dim * (2 if is_bidirectional else 1)\n",
        "        self.output_layer = nn.Linear(output_proj_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_token, hidden_state, encoder_outputs):\n",
        "        embedded = self.embedding(input_token.unsqueeze(0))  # shape: (1, batch, embed_dim)\n",
        "\n",
        "        if self.rnn_type == \"LSTM\":\n",
        "            hidden_tensor = hidden_state[0][0]\n",
        "        else:\n",
        "            hidden_tensor = hidden_state[0]\n",
        "\n",
        "        attention_input = torch.cat((embedded[0], hidden_tensor), dim=1)\n",
        "        attn_weights = F.softmax(self.attn(attention_input), dim=1)\n",
        "\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs.permute(1, 0, 2)).squeeze(1)\n",
        "        combined = torch.cat((embedded[0], context), dim=1)\n",
        "        rnn_input = F.relu(self.attn_combine(combined)).unsqueeze(0)\n",
        "\n",
        "        if self.rnn_type == \"LSTM\":\n",
        "            output, hidden = self.rnn(rnn_input, hidden_state)\n",
        "        else:\n",
        "            output, hidden = self.rnn(rnn_input, hidden_state)\n",
        "\n",
        "        logits = self.output_layer(output.squeeze(0))\n",
        "        return logits, hidden, attn_weights\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pa6t_K5_GZPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASS FOR SEQ2SEQ"
      ],
      "metadata": {
        "id": "-Upqbh1wYDB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sequence class for with attention\n",
        "class Seq2SeqAttn(pl.LightningModule):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, embed_dim, rnn_type, dropout, enc_layers, dec_layers, is_bidir, lr, max_src_len):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lr = lr\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dec_layers = dec_layers\n",
        "        self.rnn_type = rnn_type\n",
        "        self.is_bidir = is_bidir\n",
        "        self.mult = 2 if is_bidir else 1\n",
        "\n",
        "        self.encoder = AttnEncoder(input_dim, hidden_dim, embed_dim, rnn_type, dropout, enc_layers, is_bidir)\n",
        "        self.decoder = AttnDecoder(output_dim, hidden_dim, embed_dim, rnn_type, dropout, dec_layers, is_bidir, max_src_len)\n",
        "\n",
        "        self.trainLoss, self.trainAccuracy = [], []\n",
        "        self.valLoss, self.valAccuracy = [], []\n",
        "        self.test_loss, self.testAccuracy = [], []\n",
        "\n",
        "        self.attentionWeights = []\n",
        "        self.counter = 0\n",
        "\n",
        "    def forward(self, src_seq, tgt_seq, teacher_ratio=0.5):\n",
        "        batch_size, max_len = tgt_seq.shape\n",
        "        src_seq = src_seq.transpose(0, 1)\n",
        "        attn_map = torch.zeros(max_len, batch_size, self.decoder.max_seq_len).to(self.device)\n",
        "\n",
        "        vocab_size = self.decoder.output_layer.out_features\n",
        "        encoder_out, hidden = self.encoder(src_seq)\n",
        "        predictions = torch.zeros(max_len, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "        token = tgt_seq[:, 0]\n",
        "        for t in range(1, max_len):\n",
        "            pred, hidden, attn_map[t] = self.decoder(token, hidden, encoder_out)\n",
        "            predictions[t] = pred\n",
        "            token = pred.argmax(1) if teacher_ratio < torch.rand(1).item() else tgt_seq[:, t]\n",
        "\n",
        "        return predictions, attn_map\n",
        "\n",
        "    def get_model_output(self, src, tgt, teacher_ratio=0.5):\n",
        "        preds, _ = self(src, tgt, teacher_ratio)\n",
        "        return preds.permute(1, 0, 2)\n",
        "\n",
        "    def prepare_expected_tensor(self, output, target):\n",
        "        expected = torch.zeros_like(output)\n",
        "        expected[torch.arange(output.shape[0]), torch.arange(output.shape[1]).unsqueeze(1), target.cpu()] = 1\n",
        "        return expected\n",
        "\n",
        "    def prepare_output_expected_tensors(self, output, target):\n",
        "        expected = self.prepare_expected_tensor(output, target)\n",
        "        dim = output.shape[-1]\n",
        "        return output[1:].view(-1, dim), expected[1:].view(-1, dim), target[1:].view(-1)\n",
        "\n",
        "    def calculate_loss(self, preds, expected):\n",
        "        return self.loss_fn(preds.to(device), expected.to(device))\n",
        "\n",
        "    def calculate_accuracy(self, preds, target):\n",
        "        return self.accuracy(preds, target)\n",
        "\n",
        "    def append_metrics(self, loss_val, acc_val):\n",
        "        self.trainLoss.append(torch.tensor(loss_val))\n",
        "        self.trainAccuracy.append(torch.tensor(acc_val))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "\n",
        "        # 1. Forward pass  (seq_len, batch, vocab)\n",
        "        seq_logits, _ = self(src, tgt, teacher_ratio=0.5)\n",
        "\n",
        "        # 2. Accuracy wants batch-first\n",
        "        batch_logits = seq_logits.permute(1, 0, 2)         # (batch, seq, vocab)\n",
        "        acc = self.accuracy(batch_logits, tgt)\n",
        "\n",
        "        # 3. Loss helper works with seq-first\n",
        "        flat_logits, expected, _ = self.prepare_output_expected_tensors(seq_logits, tgt)\n",
        "        loss = self.loss_fn(flat_logits, expected)\n",
        "\n",
        "        # 4. Log\n",
        "        self.append_metrics(loss, acc)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "\n",
        "    def get_output(self, src, tgt):\n",
        "        raw_out, _ = self(src, tgt, 0.0)\n",
        "        acc_out, _ = self(src, tgt, 0.0)\n",
        "        return raw_out, acc_out.permute(1, 0, 2)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        raw_out, acc_out = self.get_output(src, tgt)\n",
        "        logits, expected, labels = self.prepare_output_expected_tensors(raw_out, tgt)\n",
        "        val_loss = self.calculate_loss(logits, expected)\n",
        "        val_acc = self.calculate_accuracy(acc_out, tgt)\n",
        "        assemble_assigned_generated_seq('/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv')\n",
        "        self.valLoss.append(torch.tensor(val_loss))\n",
        "        self.valAccuracy.append(torch.tensor(val_acc))\n",
        "        return {'loss': val_loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def loss_fn(self, output, target):\n",
        "        return nn.CrossEntropyLoss()(output, target).mean()\n",
        "\n",
        "    def accuracy(self, output, target):\n",
        "        pred = output.argmax(dim=-1)\n",
        "        matches = sum(torch.equal(target[i, 1:-1], pred[i, 1:-1]) for i in range(target.size(0)))\n",
        "        return 100.0 * matches / target.size(0)\n",
        "\n",
        "    def grid(self, src, output, target):\n",
        "        pred = output.argmax(dim=-1)\n",
        "        return [src[i, 1:-1] for i in range(target.size(0))], [target[i, 1:-1] for i in range(target.size(0))], [pred[i, 1:-1] for i in range(target.size(0))]\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        train_loss = torch.stack(self.trainLoss).mean()\n",
        "        val_loss = torch.stack(self.valLoss).mean()\n",
        "        train_acc = torch.stack(self.trainAccuracy).mean()\n",
        "        val_acc = torch.stack(self.valAccuracy).mean()\n",
        "        self.trainLoss.clear(), self.valLoss.clear(), self.trainAccuracy.clear(), self.valAccuracy.clear()\n",
        "        print({\"Train Loss\": round(train_loss.item(), 3), \"Train Accuracy\": round(train_acc.item(), 3), \"Validation Loss\": round(val_loss.item(), 3), \"Validation Accuracy\": round(val_acc.item(), 3)})\n",
        "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_acc, \"Validation Loss\": val_loss, \"Validation Accuracy\": val_acc})\n",
        "\n",
        "    def get_out_attention(self, src, tgt):\n",
        "        output, attn = self(src, tgt, 0.0)\n",
        "        acc_out, attn_map = self(src, tgt, 0.0)\n",
        "        return output, attn, acc_out.permute(1, 0, 2)\n",
        "\n",
        "    def get_expected(self, output, target):\n",
        "        mask = torch.zeros_like(output)\n",
        "        mask[torch.arange(output.shape[0]), torch.arange(output.shape[1]).unsqueeze(1), target.cpu()] = 1\n",
        "        return mask\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        assemble_assigned_generated_seq('/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv')\n",
        "        src, tgt = batch\n",
        "        raw_out, attn_map, acc_out = self.get_out_attention(src, tgt)\n",
        "        expected = self.get_expected(raw_out, tgt)\n",
        "\n",
        "        dim = raw_out.shape[-1]\n",
        "        logits = raw_out[1:].view(-1, dim)\n",
        "        expected = expected[1:].view(-1, dim)\n",
        "        labels = tgt[1:].view(-1)\n",
        "\n",
        "        test_loss = self.loss_fn(logits.to(device), expected.to(device))\n",
        "        test_acc = self.accuracy(acc_out, tgt)\n",
        "\n",
        "        input_tokens, target_tokens, pred_tokens = self.grid(src, acc_out, tgt)\n",
        "        assemble_assigned_generated_seq(\"string representations\")\n",
        "\n",
        "        target_texts, pred_texts, input_texts = [], [], []\n",
        "        for seq in target_tokens:\n",
        "            string = \"\".join([keyForVal(tok) for tok in seq])\n",
        "            target_texts.append(string)\n",
        "            assemble_assigned_generated_seq(string)\n",
        "\n",
        "        for seq in pred_tokens:\n",
        "            string = \"\".join([get_keyAttn(tok) for tok in seq])\n",
        "            pred_texts.append(string)\n",
        "            assemble_assigned_generated_seq(string)\n",
        "\n",
        "        for seq in input_tokens:\n",
        "            string = \"\".join([keyForInput(tok) for tok in seq])\n",
        "            input_texts.append(string)\n",
        "        assemble_assigned_generated_seq(string)\n",
        "\n",
        "        self.testAccuracy.append(torch.tensor(test_acc))\n",
        "        self.test_loss.append(torch.tensor(test_loss))\n",
        "        # print({\"for batch test_loss\": test_loss, \"testAccuracy\": test_acc})\n",
        "        # wandb.log({\"Test Loss\": test_loss, \"Test Accuracy\": test_acc})\n",
        "\n",
        "        save_outputs_to_csv(input_texts, target_texts, pred_texts)\n",
        "        # log_predictions_to_wandb(inputs, target_outputs, predicted_outputs)\n",
        "        if(self.counter<1):\n",
        "          s(input_texts, pred_texts, attn_map)\n",
        "          self.counter=self.counter+1\n",
        "\n",
        "        return {'loss': test_loss}\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        test_loss = torch.stack(self.test_loss).mean()\n",
        "        test_acc = torch.stack(self.testAccuracy).mean()\n",
        "        self.test_loss.clear()\n",
        "        self.testAccuracy.clear()\n",
        "        print({\"test_loss\": test_loss, \"testAccuracy\": test_acc})\n",
        "        wandb.log({\"test_loss_last\":test_loss,\"testAccuracy_last\":test_acc})\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8LoWl4cJXn7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to return key for any value\n",
        "def get_keyAttn(index_value):\n",
        "    for character, idx in charToIndLang.items():\n",
        "        if idx == index_value:\n",
        "            return character\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def keyForInput(index):\n",
        "    for char, idx in char_to_idx_latin.items():\n",
        "        if idx == index:\n",
        "            return char\n",
        "    return \"\"\n",
        "\n",
        "def keyForVal(index):\n",
        "    for char, idx in charToIndLang.items():\n",
        "        if idx == index:\n",
        "            return char\n",
        "    return \"\"\n",
        "\n",
        "def get_key_inputAttn(index_value):\n",
        "    for char, idx in char_to_idx_latin.items():\n",
        "        if idx == index_value:\n",
        "            return char\n",
        "    return \"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "9RmeH7MKXtkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function will save the input word ,ouput word and predicted word to csv file for attention module\n",
        "def save_outputs_to_csv(inputs, targets, predictions):\n",
        "    output_file = 'Output.csv'\n",
        "    already_present = os.path.isfile(output_file)\n",
        "\n",
        "    records = {\n",
        "        'input': inputs,\n",
        "        'target': targets,\n",
        "        'predicted': predictions\n",
        "    }\n",
        "\n",
        "    dataframe = pd.DataFrame(records)\n",
        "    dataframe.to_csv(output_file, mode='a', index=False, header=not already_present)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def log_predictions_to_wandb(inputs, targets, predictions):\n",
        "    table = wandb.Table(columns=[\"Input\", \"Target\", \"Prediction ✅/❌\"])\n",
        "\n",
        "    for inp, tgt, pred in zip(inputs, targets, predictions):\n",
        "        status_emoji = \"✅\" if tgt == pred else \"❌\"\n",
        "        pred_with_emoji = f\"{pred} {status_emoji}\"\n",
        "        table.add_data(inp, tgt, pred_with_emoji)\n",
        "\n",
        "    wandb.log({\"Test Predictions Grid\": table})\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Duhy70HvXuWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def s(input_words, output_words, attentionWeights):\n",
        "    \"\"\"Visualise up to 9 attention maps (3×3 grid).\"\"\"\n",
        "    dev_font = FontProperties(fname=\"/kaggle/input/devnagri/Nirmala.ttf\")\n",
        "\n",
        "    fig, axarr = plt.subplots(3, 3, figsize=(10, 10))\n",
        "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "    n_plots = min(len(input_words), len(output_words), len(attentionWeights))\n",
        "\n",
        "    for idx, ax in enumerate(axarr.ravel()):\n",
        "        if idx >= n_plots:           # blank panel\n",
        "            ax.axis(\"off\")\n",
        "            continue\n",
        "\n",
        "        # ─────── extract matrix for this pair ───────────────────────────────\n",
        "        attn = attentionWeights[idx].cpu().detach().numpy()\n",
        "        attn = attn[1 : len(input_words[idx]) + 1, : len(output_words[idx])]\n",
        "\n",
        "        # ─────── heat-map ───────────────────────────────────────────────────\n",
        "        sns.heatmap(\n",
        "            attn,\n",
        "            ax=ax,\n",
        "            cmap=\"viridis\",          # purple → green → yellow\n",
        "            cbar=False,\n",
        "            square=True,\n",
        "            vmin=0,\n",
        "            vmax=attn.max(),\n",
        "        )\n",
        "\n",
        "        # dashed white grid\n",
        "        ax.set_xticks(np.arange(-0.5, attn.shape[1], 1), minor=True)\n",
        "        ax.set_yticks(np.arange(-0.5, attn.shape[0], 1), minor=True)\n",
        "        ax.grid(which=\"minor\", color=\"white\", linestyle=\"--\", linewidth=0.5)\n",
        "        ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "        # ─────── ticks / labels ─────────────────────────────────────────────\n",
        "        ax.xaxis.tick_top()  # move x-labels to top\n",
        "        ax.set_xticks(range(len(output_words[idx])))\n",
        "        ax.set_xticklabels(\n",
        "            reversed(output_words[idx]),\n",
        "            rotation=45,\n",
        "            ha=\"right\",\n",
        "            fontproperties=dev_font,\n",
        "            fontsize=9,\n",
        "        )\n",
        "\n",
        "        ax.set_yticks(range(len(input_words[idx])))\n",
        "        ax.set_yticklabels(\n",
        "            reversed(input_words[idx]),\n",
        "            rotation=\"vertical\",\n",
        "            fontproperties=dev_font,\n",
        "            fontsize=9,\n",
        "        )\n",
        "\n",
        "        ax.set_title(f\"Attention {idx + 1}\", fontsize=12)\n",
        "\n",
        "    # log and display\n",
        "    wandb.log({\"Question5_HeatMap\": wandb.Image(fig)})\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n"
      ],
      "metadata": {
        "id": "jsAqplLVX-Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SWEEP CONFIG"
      ],
      "metadata": {
        "id": "bwsU-s16YO57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    # Bayesian Search for hyperparameters\n",
        "    \"name\" : \"Bayesian Sweep_Attn\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Validation Accuracy\"},\n",
        "    \"parameters\": {'drop_out': {\"values\": [0.3,0.5]},\n",
        "                   'embeddingSize': {\"values\": [64,128,256]},\n",
        "                   'hidden_layer_size': {\"values\": [128,256,512]},\n",
        "                   'layersEncoder': {\"values\": [2, 3]},\n",
        "                   'layersDecoder': {\"values\": [2, 3]},\n",
        "                   \"cellType\": {\"values\": [ \"RNN\", \"GRU\", \"LSTM\"]},\n",
        "                   \"learningRate\": {\"values\": [1e-3, 0.005]},\n",
        "                   \"bidirectional\":{\"values\":[True, False]},\n",
        "                   \"epochs\": {\"values\": [10, 15]}\n",
        "                }\n",
        "}"
      ],
      "metadata": {
        "id": "_xsBDh6XYJd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    run = wandb.init(project=\"DA6401_ASS3_ATTENTION\", name=\"SweepAttention\")\n",
        "    config = run.config\n",
        "\n",
        "    # Construct a dynamic run name\n",
        "    wandb.run.name = (\n",
        "        f\"_cell_{config.cellType}_dr_{config.drop_out}\"\n",
        "        f\"_em_{config.embeddingSize}_hl_{config.hidden_layer_size}\"\n",
        "        f\"_en_{config.layersEncoder}_de_{config.layersDecoder}\"\n",
        "        f\"_lr_{config.learningRate}_bi_{config.bidirectional}\"\n",
        "        f\"_ep_{config.epochs}\"\n",
        "    )\n",
        "\n",
        "    # Extract hyperparameters\n",
        "    hidden_size = config.hidden_layer_size\n",
        "    embed_dim = config.embeddingSize\n",
        "    rnn_type = config.cellType\n",
        "    dec_layers = config.layersDecoder\n",
        "    enc_layers = config.layersEncoder\n",
        "    is_bidir = config.bidirectional\n",
        "    learning_rate = config.learningRate\n",
        "    dropout = config.drop_out\n",
        "    num_epochs = config.epochs\n",
        "    use_attention = True\n",
        "\n",
        "    input_vocab_size = len(char_to_idx_latin) + 2\n",
        "    output_vocab_size = len(charToIndLang) + 2\n",
        "\n",
        "    if not use_attention:\n",
        "        model = Seq2Seq(\n",
        "            input_vocab_size,\n",
        "            output_vocab_size,\n",
        "            hidden_size,\n",
        "            embed_dim,\n",
        "            rnn_type,\n",
        "            dropout,\n",
        "            enc_layers,\n",
        "            dec_layers,\n",
        "            is_bidir,\n",
        "            learning_rate\n",
        "        )\n",
        "    else:\n",
        "        model = Seq2SeqAttn(\n",
        "            input_vocab_size,\n",
        "            output_vocab_size,\n",
        "            hidden_size,\n",
        "            embed_dim,\n",
        "            rnn_type,\n",
        "            dropout,\n",
        "            enc_layers=1,\n",
        "            dec_layers=1,\n",
        "            is_bidir=is_bidir,\n",
        "            lr=learning_rate,\n",
        "            max_src_len=maxLenEng\n",
        "        )\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        accelerator=\"gpu\",\n",
        "        devices=1\n",
        "    )\n",
        "    trainer.fit(model, train_dataloaders=dataloaderTrain, val_dataloaders=dataloaderVal)\n"
      ],
      "metadata": {
        "id": "klZAfJF5YRfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Running Sweeps for with attention\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_ASS3_ATTENTION')\n",
        "wandb.agent(sweep_id, train, count = 23)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "z1EPNstyYUEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BEST CONFIG"
      ],
      "metadata": {
        "id": "yZ--WeyMYXNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs=15\n",
        "\n",
        "hidden_layer_size=256\n",
        "embeddingSize= 128\n",
        "cellType=\"LSTM\"\n",
        "layersDecoder=3\n",
        "layersEncoder=3\n",
        "attention=True\n",
        "bidirectional=True\n",
        "learningRate=0.001\n",
        "drop_out=0.5\n"
      ],
      "metadata": {
        "id": "E1HpgFyiYZ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Seq2SeqAttn(len(char_to_idx_latin)+2, len(charToIndLang)+2, hidden_layer_size, embeddingSize, cellType,drop_out,3,3,bidirectional,learningRate, maxLenEng)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "SeM1kb6iYcg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"DA6401_ASS3_ATTENTION\",name=\"SweepAttention\")\n",
        "trainer = pl.Trainer(max_epochs=epochs, accelerator=\"gpu\", devices=1)\n",
        "trainer.fit(model=model, train_dataloaders=dataloaderTrain, val_dataloaders=dataloaderVal)\n",
        "trainer.test(model, dataloaderTest)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "aO3vHr84YfLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}